========================
Logs for Query 'use hw6'
========================


================================================================================================================================================================================================================================================================================================================================================================================================
Logs for Query 'SELECT city_id, name AS city_name, type AS most_popular_ua_type, cnt AS count
FROM
(
  SELECT t1.city_id, t1.type, t1.cnt, rank() OVER (PARTITION BY t1.city_id ORDER BY t1.cnt DESC) AS rnk
  FROM
  (
    SELECT city_id, type, COUNT(*) AS cnt
    FROM city_ua
    GROUP BY city_id, type
  ) t1
) t2
LEFT JOIN city ON t2.city_id = city.id
WHERE rnk = 1
ORDER BY city_id'
================================================================================================================================================================================================================================================================================================================================================================================================

INFO  : Number of reduce tasks not specified. Estimated from input data size: 3
INFO  : In order to change the average load for a reducer (in bytes):
INFO  :   set hive.exec.reducers.bytes.per.reducer=<number>
INFO  : In order to limit the maximum number of reducers:
INFO  :   set hive.exec.reducers.max=<number>
INFO  : In order to set a constant number of reducers:
INFO  :   set mapreduce.job.reduces=<number>
INFO  : number of splits:1
INFO  : Submitting tokens for job: job_1518505142614_0024
INFO  : The url to track the job: http://sandbox-hdp.hortonworks.com:8088/proxy/application_1518505142614_0024/
INFO  : Starting Job = job_1518505142614_0024, Tracking URL = http://sandbox-hdp.hortonworks.com:8088/proxy/application_1518505142614_0024/
INFO  : Kill Command = /usr/hdp/2.6.3.0-235/hadoop/bin/hadoop job  -kill job_1518505142614_0024
INFO  : Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 3
INFO  : 2018-02-13 07:48:22,801 Stage-1 map = 0%,  reduce = 0%
INFO  : 2018-02-13 07:48:28,985 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 4.92 sec
INFO  : 2018-02-13 07:48:35,270 Stage-1 map = 100%,  reduce = 33%, Cumulative CPU 7.34 sec
INFO  : 2018-02-13 07:48:36,304 Stage-1 map = 100%,  reduce = 67%, Cumulative CPU 9.71 sec
INFO  : 2018-02-13 07:48:37,327 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 12.1 sec
INFO  : MapReduce Total cumulative CPU time: 12 seconds 100 msec
INFO  : Ended Job = job_1518505142614_0024
INFO  : Number of reduce tasks not specified. Estimated from input data size: 1
INFO  : In order to change the average load for a reducer (in bytes):
INFO  :   set hive.exec.reducers.bytes.per.reducer=<number>
INFO  : In order to limit the maximum number of reducers:
INFO  :   set hive.exec.reducers.max=<number>
INFO  : In order to set a constant number of reducers:
INFO  :   set mapreduce.job.reduces=<number>
INFO  : number of splits:1
INFO  : Submitting tokens for job: job_1518505142614_0025
INFO  : The url to track the job: http://sandbox-hdp.hortonworks.com:8088/proxy/application_1518505142614_0025/
INFO  : Starting Job = job_1518505142614_0025, Tracking URL = http://sandbox-hdp.hortonworks.com:8088/proxy/application_1518505142614_0025/
INFO  : Kill Command = /usr/hdp/2.6.3.0-235/hadoop/bin/hadoop job  -kill job_1518505142614_0025
INFO  : Hadoop job information for Stage-2: number of mappers: 1; number of reducers: 1
INFO  : 2018-02-13 07:48:44,460 Stage-2 map = 0%,  reduce = 0%
INFO  : 2018-02-13 07:48:49,635 Stage-2 map = 100%,  reduce = 0%, Cumulative CPU 1.09 sec
INFO  : 2018-02-13 07:48:54,802 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 3.53 sec
INFO  : MapReduce Total cumulative CPU time: 3 seconds 530 msec
INFO  : Ended Job = job_1518505142614_0025
INFO  : Execution completed successfully
INFO  : MapredLocal task succeeded
INFO  : Number of reduce tasks determined at compile time: 1
INFO  : In order to change the average load for a reducer (in bytes):
INFO  :   set hive.exec.reducers.bytes.per.reducer=<number>
INFO  : In order to limit the maximum number of reducers:
INFO  :   set hive.exec.reducers.max=<number>
INFO  : In order to set a constant number of reducers:
INFO  :   set mapreduce.job.reduces=<number>
INFO  : number of splits:1
INFO  : Submitting tokens for job: job_1518505142614_0026
INFO  : The url to track the job: http://sandbox-hdp.hortonworks.com:8088/proxy/application_1518505142614_0026/
INFO  : Starting Job = job_1518505142614_0026, Tracking URL = http://sandbox-hdp.hortonworks.com:8088/proxy/application_1518505142614_0026/
INFO  : Kill Command = /usr/hdp/2.6.3.0-235/hadoop/bin/hadoop job  -kill job_1518505142614_0026
INFO  : Hadoop job information for Stage-4: number of mappers: 1; number of reducers: 1
INFO  : 2018-02-13 07:49:04,182 Stage-4 map = 0%,  reduce = 0%
INFO  : 2018-02-13 07:49:08,339 Stage-4 map = 100%,  reduce = 0%, Cumulative CPU 2.1 sec
INFO  : 2018-02-13 07:49:13,503 Stage-4 map = 100%,  reduce = 100%, Cumulative CPU 4.46 sec
INFO  : MapReduce Total cumulative CPU time: 4 seconds 460 msec
INFO  : Ended Job = job_1518505142614_0026