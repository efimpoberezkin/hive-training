========================
Logs for Query 'use hw6'
========================


=======================================================================================================================================================================================================================================================================================================================================================================================================
Logs for Query 'SELECT city_id, name AS city_name, device AS most_popular_device, cnt AS count
FROM
(
  SELECT t1.city_id, t1.device, t1.cnt, rank() OVER (PARTITION BY t1.city_id ORDER BY t1.cnt DESC) AS rnk
  FROM
  (
    SELECT city_id, device, COUNT(*) AS cnt
    FROM city_ua
    GROUP BY city_id, device
  ) t1
) t2
LEFT JOIN city ON t2.city_id = city.id
WHERE rnk = 1
ORDER BY city_id'
=======================================================================================================================================================================================================================================================================================================================================================================================================

INFO  : Number of reduce tasks not specified. Estimated from input data size: 3
INFO  : In order to change the average load for a reducer (in bytes):
INFO  :   set hive.exec.reducers.bytes.per.reducer=<number>
INFO  : In order to limit the maximum number of reducers:
INFO  :   set hive.exec.reducers.max=<number>
INFO  : In order to set a constant number of reducers:
INFO  :   set mapreduce.job.reduces=<number>
INFO  : number of splits:1
INFO  : Submitting tokens for job: job_1518505142614_0015
INFO  : The url to track the job: http://sandbox-hdp.hortonworks.com:8088/proxy/application_1518505142614_0015/
INFO  : Starting Job = job_1518505142614_0015, Tracking URL = http://sandbox-hdp.hortonworks.com:8088/proxy/application_1518505142614_0015/
INFO  : Kill Command = /usr/hdp/2.6.3.0-235/hadoop/bin/hadoop job  -kill job_1518505142614_0015
INFO  : Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 3
INFO  : 2018-02-13 07:35:28,581 Stage-1 map = 0%,  reduce = 0%
INFO  : 2018-02-13 07:35:35,819 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 4.82 sec
INFO  : 2018-02-13 07:35:42,129 Stage-1 map = 100%,  reduce = 33%, Cumulative CPU 7.25 sec
INFO  : 2018-02-13 07:35:43,156 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 10.78 sec
INFO  : MapReduce Total cumulative CPU time: 10 seconds 780 msec
INFO  : Ended Job = job_1518505142614_0015
INFO  : Number of reduce tasks not specified. Estimated from input data size: 1
INFO  : In order to change the average load for a reducer (in bytes):
INFO  :   set hive.exec.reducers.bytes.per.reducer=<number>
INFO  : In order to limit the maximum number of reducers:
INFO  :   set hive.exec.reducers.max=<number>
INFO  : In order to set a constant number of reducers:
INFO  :   set mapreduce.job.reduces=<number>
INFO  : number of splits:1
INFO  : Submitting tokens for job: job_1518505142614_0016
INFO  : The url to track the job: http://sandbox-hdp.hortonworks.com:8088/proxy/application_1518505142614_0016/
INFO  : Starting Job = job_1518505142614_0016, Tracking URL = http://sandbox-hdp.hortonworks.com:8088/proxy/application_1518505142614_0016/
INFO  : Kill Command = /usr/hdp/2.6.3.0-235/hadoop/bin/hadoop job  -kill job_1518505142614_0016
INFO  : Hadoop job information for Stage-2: number of mappers: 1; number of reducers: 1
INFO  : 2018-02-13 07:35:50,852 Stage-2 map = 0%,  reduce = 0%
INFO  : 2018-02-13 07:35:55,058 Stage-2 map = 100%,  reduce = 0%, Cumulative CPU 1.08 sec
INFO  : 2018-02-13 07:36:01,311 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 3.94 sec
INFO  : MapReduce Total cumulative CPU time: 3 seconds 940 msec
INFO  : Ended Job = job_1518505142614_0016
INFO  : Execution completed successfully
INFO  : MapredLocal task succeeded
INFO  : Number of reduce tasks determined at compile time: 1
INFO  : In order to change the average load for a reducer (in bytes):
INFO  :   set hive.exec.reducers.bytes.per.reducer=<number>
INFO  : In order to limit the maximum number of reducers:
INFO  :   set hive.exec.reducers.max=<number>
INFO  : In order to set a constant number of reducers:
INFO  :   set mapreduce.job.reduces=<number>
INFO  : number of splits:1
INFO  : Submitting tokens for job: job_1518505142614_0017
INFO  : The url to track the job: http://sandbox-hdp.hortonworks.com:8088/proxy/application_1518505142614_0017/
INFO  : Starting Job = job_1518505142614_0017, Tracking URL = http://sandbox-hdp.hortonworks.com:8088/proxy/application_1518505142614_0017/
INFO  : Kill Command = /usr/hdp/2.6.3.0-235/hadoop/bin/hadoop job  -kill job_1518505142614_0017
INFO  : Hadoop job information for Stage-4: number of mappers: 1; number of reducers: 1
INFO  : 2018-02-13 07:36:12,278 Stage-4 map = 0%,  reduce = 0%
INFO  : 2018-02-13 07:36:16,436 Stage-4 map = 100%,  reduce = 0%, Cumulative CPU 1.4 sec
INFO  : 2018-02-13 07:36:21,650 Stage-4 map = 100%,  reduce = 100%, Cumulative CPU 2.98 sec
INFO  : MapReduce Total cumulative CPU time: 2 seconds 980 msec
INFO  : Ended Job = job_1518505142614_0017