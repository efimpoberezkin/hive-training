========================
Logs for Query 'use hw6'
========================


===================================================================================================================================================================================================================================================================================================================================================================================
Logs for Query 'SELECT city_id, name AS city_name, os AS most_popular_os, cnt AS count
FROM
(
  SELECT t1.city_id, t1.os, t1.cnt, rank() OVER (PARTITION BY t1.city_id ORDER BY t1.cnt DESC) AS rnk
  FROM
  (
    SELECT city_id, os, COUNT(*) AS cnt
    FROM city_ua
    GROUP BY city_id, os
  ) t1
) t2
LEFT JOIN city ON t2.city_id = city.id
WHERE rnk = 1
ORDER BY city_id'
===================================================================================================================================================================================================================================================================================================================================================================================

INFO  : Number of reduce tasks not specified. Estimated from input data size: 3
INFO  : In order to change the average load for a reducer (in bytes):
INFO  :   set hive.exec.reducers.bytes.per.reducer=<number>
INFO  : In order to limit the maximum number of reducers:
INFO  :   set hive.exec.reducers.max=<number>
INFO  : In order to set a constant number of reducers:
INFO  :   set mapreduce.job.reduces=<number>
INFO  : number of splits:1
INFO  : Submitting tokens for job: job_1518505142614_0021
INFO  : The url to track the job: http://sandbox-hdp.hortonworks.com:8088/proxy/application_1518505142614_0021/
INFO  : Starting Job = job_1518505142614_0021, Tracking URL = http://sandbox-hdp.hortonworks.com:8088/proxy/application_1518505142614_0021/
INFO  : Kill Command = /usr/hdp/2.6.3.0-235/hadoop/bin/hadoop job  -kill job_1518505142614_0021
INFO  : Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 3
INFO  : 2018-02-13 07:45:19,214 Stage-1 map = 0%,  reduce = 0%
INFO  : 2018-02-13 07:45:25,384 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 4.91 sec
INFO  : 2018-02-13 07:45:31,644 Stage-1 map = 100%,  reduce = 33%, Cumulative CPU 7.31 sec
INFO  : 2018-02-13 07:45:32,675 Stage-1 map = 100%,  reduce = 67%, Cumulative CPU 9.38 sec
INFO  : 2018-02-13 07:45:33,728 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 11.39 sec
INFO  : MapReduce Total cumulative CPU time: 11 seconds 390 msec
INFO  : Ended Job = job_1518505142614_0021
INFO  : Number of reduce tasks not specified. Estimated from input data size: 1
INFO  : In order to change the average load for a reducer (in bytes):
INFO  :   set hive.exec.reducers.bytes.per.reducer=<number>
INFO  : In order to limit the maximum number of reducers:
INFO  :   set hive.exec.reducers.max=<number>
INFO  : In order to set a constant number of reducers:
INFO  :   set mapreduce.job.reduces=<number>
INFO  : number of splits:1
INFO  : Submitting tokens for job: job_1518505142614_0022
INFO  : The url to track the job: http://sandbox-hdp.hortonworks.com:8088/proxy/application_1518505142614_0022/
INFO  : Starting Job = job_1518505142614_0022, Tracking URL = http://sandbox-hdp.hortonworks.com:8088/proxy/application_1518505142614_0022/
INFO  : Kill Command = /usr/hdp/2.6.3.0-235/hadoop/bin/hadoop job  -kill job_1518505142614_0022
INFO  : Hadoop job information for Stage-2: number of mappers: 1; number of reducers: 1
INFO  : 2018-02-13 07:45:41,031 Stage-2 map = 0%,  reduce = 0%
INFO  : 2018-02-13 07:45:45,197 Stage-2 map = 100%,  reduce = 0%, Cumulative CPU 1.68 sec
INFO  : 2018-02-13 07:45:50,391 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 4.24 sec
INFO  : MapReduce Total cumulative CPU time: 4 seconds 240 msec
INFO  : Ended Job = job_1518505142614_0022
INFO  : Execution completed successfully
INFO  : MapredLocal task succeeded
INFO  : Number of reduce tasks determined at compile time: 1
INFO  : In order to change the average load for a reducer (in bytes):
INFO  :   set hive.exec.reducers.bytes.per.reducer=<number>
INFO  : In order to limit the maximum number of reducers:
INFO  :   set hive.exec.reducers.max=<number>
INFO  : In order to set a constant number of reducers:
INFO  :   set mapreduce.job.reduces=<number>
INFO  : number of splits:1
INFO  : Submitting tokens for job: job_1518505142614_0023
INFO  : The url to track the job: http://sandbox-hdp.hortonworks.com:8088/proxy/application_1518505142614_0023/
INFO  : Starting Job = job_1518505142614_0023, Tracking URL = http://sandbox-hdp.hortonworks.com:8088/proxy/application_1518505142614_0023/
INFO  : Kill Command = /usr/hdp/2.6.3.0-235/hadoop/bin/hadoop job  -kill job_1518505142614_0023
INFO  : Hadoop job information for Stage-4: number of mappers: 1; number of reducers: 1
INFO  : 2018-02-13 07:46:00,439 Stage-4 map = 0%,  reduce = 0%
INFO  : 2018-02-13 07:46:04,563 Stage-4 map = 100%,  reduce = 0%, Cumulative CPU 1.56 sec
INFO  : 2018-02-13 07:46:10,783 Stage-4 map = 100%,  reduce = 100%, Cumulative CPU 3.85 sec
INFO  : MapReduce Total cumulative CPU time: 3 seconds 850 msec
INFO  : Ended Job = job_1518505142614_0023