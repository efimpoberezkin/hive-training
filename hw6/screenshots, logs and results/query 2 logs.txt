========================
Logs for Query 'use hw6'
========================


============================================================================================================================================================================================================================================================================================================================================================================================================
Logs for Query 'SELECT city_id, name AS city_name, browser AS most_popular_browser, cnt AS count
FROM
(
  SELECT t1.city_id, t1.browser, t1.cnt, rank() OVER (PARTITION BY t1.city_id ORDER BY t1.cnt DESC) AS rnk
  FROM
  (
    SELECT city_id, browser, COUNT(*) AS cnt
    FROM city_ua
    GROUP BY city_id, browser
  ) t1
) t2
LEFT JOIN city ON t2.city_id = city.id
WHERE rnk = 1
ORDER BY city_id'
============================================================================================================================================================================================================================================================================================================================================================================================================

INFO  : Number of reduce tasks not specified. Estimated from input data size: 3
INFO  : In order to change the average load for a reducer (in bytes):
INFO  :   set hive.exec.reducers.bytes.per.reducer=<number>
INFO  : In order to limit the maximum number of reducers:
INFO  :   set hive.exec.reducers.max=<number>
INFO  : In order to set a constant number of reducers:
INFO  :   set mapreduce.job.reduces=<number>
INFO  : number of splits:1
INFO  : Submitting tokens for job: job_1518505142614_0018
INFO  : The url to track the job: http://sandbox-hdp.hortonworks.com:8088/proxy/application_1518505142614_0018/
INFO  : Starting Job = job_1518505142614_0018, Tracking URL = http://sandbox-hdp.hortonworks.com:8088/proxy/application_1518505142614_0018/
INFO  : Kill Command = /usr/hdp/2.6.3.0-235/hadoop/bin/hadoop job  -kill job_1518505142614_0018
INFO  : Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 3
INFO  : 2018-02-13 07:41:00,807 Stage-1 map = 0%,  reduce = 0%
INFO  : 2018-02-13 07:41:08,054 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 5.05 sec
INFO  : 2018-02-13 07:41:14,457 Stage-1 map = 100%,  reduce = 33%, Cumulative CPU 7.08 sec
INFO  : 2018-02-13 07:41:15,557 Stage-1 map = 100%,  reduce = 67%, Cumulative CPU 8.74 sec
INFO  : 2018-02-13 07:41:16,597 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 10.8 sec
INFO  : MapReduce Total cumulative CPU time: 10 seconds 800 msec
INFO  : Ended Job = job_1518505142614_0018
INFO  : Number of reduce tasks not specified. Estimated from input data size: 1
INFO  : In order to change the average load for a reducer (in bytes):
INFO  :   set hive.exec.reducers.bytes.per.reducer=<number>
INFO  : In order to limit the maximum number of reducers:
INFO  :   set hive.exec.reducers.max=<number>
INFO  : In order to set a constant number of reducers:
INFO  :   set mapreduce.job.reduces=<number>
INFO  : number of splits:1
INFO  : Submitting tokens for job: job_1518505142614_0019
INFO  : The url to track the job: http://sandbox-hdp.hortonworks.com:8088/proxy/application_1518505142614_0019/
INFO  : Starting Job = job_1518505142614_0019, Tracking URL = http://sandbox-hdp.hortonworks.com:8088/proxy/application_1518505142614_0019/
INFO  : Kill Command = /usr/hdp/2.6.3.0-235/hadoop/bin/hadoop job  -kill job_1518505142614_0019
INFO  : Hadoop job information for Stage-2: number of mappers: 1; number of reducers: 1
INFO  : 2018-02-13 07:41:23,986 Stage-2 map = 0%,  reduce = 0%
INFO  : 2018-02-13 07:41:28,140 Stage-2 map = 100%,  reduce = 0%, Cumulative CPU 1.85 sec
INFO  : 2018-02-13 07:41:34,357 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 4.75 sec
INFO  : MapReduce Total cumulative CPU time: 4 seconds 750 msec
INFO  : Ended Job = job_1518505142614_0019
INFO  : Execution completed successfully
INFO  : MapredLocal task succeeded
INFO  : Number of reduce tasks determined at compile time: 1
INFO  : In order to change the average load for a reducer (in bytes):
INFO  :   set hive.exec.reducers.bytes.per.reducer=<number>
INFO  : In order to limit the maximum number of reducers:
INFO  :   set hive.exec.reducers.max=<number>
INFO  : In order to set a constant number of reducers:
INFO  :   set mapreduce.job.reduces=<number>
INFO  : number of splits:1
INFO  : Submitting tokens for job: job_1518505142614_0020
INFO  : The url to track the job: http://sandbox-hdp.hortonworks.com:8088/proxy/application_1518505142614_0020/
INFO  : Starting Job = job_1518505142614_0020, Tracking URL = http://sandbox-hdp.hortonworks.com:8088/proxy/application_1518505142614_0020/
INFO  : Kill Command = /usr/hdp/2.6.3.0-235/hadoop/bin/hadoop job  -kill job_1518505142614_0020
INFO  : Hadoop job information for Stage-4: number of mappers: 1; number of reducers: 1
INFO  : 2018-02-13 07:41:44,836 Stage-4 map = 0%,  reduce = 0%
INFO  : 2018-02-13 07:41:49,058 Stage-4 map = 100%,  reduce = 0%, Cumulative CPU 1.52 sec
INFO  : 2018-02-13 07:41:54,252 Stage-4 map = 100%,  reduce = 100%, Cumulative CPU 3.52 sec
INFO  : MapReduce Total cumulative CPU time: 3 seconds 520 msec
INFO  : Ended Job = job_1518505142614_0020